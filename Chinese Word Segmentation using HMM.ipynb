{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confirmed-cycling",
   "metadata": {},
   "source": [
    "# Chinese Word Segmentation using HMM\n",
    "By: [Richard Cornelius Suwandi](https://richardcsuwandi.github.io)\n",
    "\n",
    "In this project, we are going to perform word segmentation using the Hidden Markov Model (HMM) on the given dataset. The dataset contains 5500 sentences for training and 1492 sentences for testing, all of them written in Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tropical-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "# Use regular expression to detect Chinese characters\n",
    "regex = re.compile(\"[^\\u4e00-\\u9fff]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-memphis",
   "metadata": {},
   "source": [
    "We first load the training and testing data, as well as the \"gold standard\" (ground truth):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "attached-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and labels\n",
    "train_data = open(os.path.join(\"data\", \"train.txt\"), encoding=\"utf-8-sig\").read().splitlines()\n",
    "test_data = open(os.path.join(\"data\", \"test.txt\"), encoding=\"utf-8-sig\").read().splitlines()\n",
    "test_gold = open(os.path.join(\"data\", \"test_gold.txt\"), encoding=\"utf-8-sig\").read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-apartment",
   "metadata": {},
   "source": [
    "The training data is stored in a list, where each element of the list corresponds to a sentence. Let's take a look at some training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "noble-violin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['立會 選情 告一段落 民主 進程 還 看 明天',\n",
       " '立法會 選舉 出現 了 戲劇性 的 結果 ， 儘管 投票率 創下 新高 ， 而 過去 經驗 顯示 高 投票率 對 民主派 較 有利 ， 但 由於 名單 協調 不當 及 配票 策略 失誤 ， 加上 醜聞 影響 選情 ， 民主黨 的 議席 比 上 一 屆 減少 ， 由 第 一 大 黨 跌 至 第 三 ；',\n",
       " '而 泛民主派 在 30 席 普選 中 亦 只能 取得 18 席 ， 比 選前 預期 的 20 席 少 ；',\n",
       " '但 在 功能 組別 選舉 卻 有 意外 收穫 ， 除 保住 原有 的 5 個 議席 ， 還 搶佔 了 醫學 和 會計 兩 個 專業 界別 ， 令 議席 總數 達到 25 席 ， 比 上 一 屆 多 了 3 席 。',\n",
       " '更 值得 注意 的 是 ， 泛民主派 候選人 在 普選 中 合共 取得 110萬 張 選票 ， 佔 178萬 選票 總數 的 62 ％ ， 顯示 多數 市民 認同 早日 實現 全面 普選 的 民主 訴求 ， 這 一 點 應 為 政府 及 各 黨派 人士 所 尊重 。']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print some training examples\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-interface",
   "metadata": {},
   "source": [
    "As we can see, the sentences in the training data have already been segmented according to their labels. In particular, there are 4 labels (hidden states) for Chinese word segmentation:\n",
    "1. 'B': Beginning character \n",
    "2. 'I': Intermediate (internal) character\n",
    "3. 'E': Ending character\n",
    "4. 'S': Single character\n",
    "\n",
    "For example, the sentence '立會 選情 告一段落 民主 進程 還 看 明天' would correspond to the following labels: 'BE BE BIIE BE BE BE S S BE'. Hence, we first need to preprocess our data and create the hidden states for each sentence. We will define a helper function to preprocess both the training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "nearby-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    hidden_state_lst = []\n",
    "    data_lst = []\n",
    "    word_count = {}\n",
    "\n",
    "    for sentence in data:\n",
    "        hidden_state = \"\"\n",
    "        words = []\n",
    "        for token in sentence.split():\n",
    "            token = regex.sub(\"\", token)\n",
    "            # Label the token according to its length\n",
    "            if len(token) == 1:\n",
    "                hidden_state += \"S\"\n",
    "            elif len(token) == 2:\n",
    "                hidden_state += \"BE\"\n",
    "            elif len(token) > 2:\n",
    "                hidden_state += \"B\" + (len(token) - 2)*\"I\" + \"E\"\n",
    "\n",
    "        # Remove the spaces between characters\n",
    "        sentence = sentence.replace(\" \", \"\")\n",
    "        for word in sentence:\n",
    "            # Verify if the word is a Chinese character\n",
    "            word = regex.sub(\"\", word)\n",
    "            # Update the word count dictionary\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "            words.append(word)\n",
    "        if len(words) > 0:\n",
    "            data_lst.append(words)\n",
    "            states = [s for s in hidden_state]\n",
    "            hidden_state_lst.append(states)\n",
    "\n",
    "    return data_lst, hidden_state_lst, word_count\n",
    "\n",
    "train_data, train_hs, train_wc = preprocess(train_data)\n",
    "test_data, _, test_wc = preprocess(test_data)\n",
    "_, test_hs, _ = preprocess(test_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-powell",
   "metadata": {},
   "source": [
    "Notice that we also created a dictionary to store the word count, this will be handy for computing the probabilities. In HMM, we need construct the start/initial state matrix, the transition matrix, and the emission matrix. The start probabilities for the initial state matrix can be simply calculated as:\n",
    "\\begin{align}\n",
    "    P(\\text{State } k) = \\frac{\\text{The frequency of state } k}{\\text{The total number of states}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "australian-cruise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': 0.3575882854480201, 'I': 0.0595393888921936, 'E': 0.3575882854480201, 'S': 0.2252840402117661}\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store the state count\n",
    "state_count = {}\n",
    "states = [\"B\", \"I\", \"E\", \"S\"]\n",
    "for state in states: \n",
    "    state_count[state] = 0\n",
    "\n",
    "for i in range(len(train_hs)):\n",
    "    length = len(train_hs[i])\n",
    "    if length > 0:\n",
    "        for j in range(length - 1):\n",
    "            # Update the state count\n",
    "            state_count[train_hs[i][j]] += 1\n",
    "        state_count[train_hs[i][length - 1]] += 1\n",
    "\n",
    "total_states = sum(state_count.values())  # Get the total number of states in the sample\n",
    "start_prob = {}\n",
    "\n",
    "for state in states:\n",
    "    # Normalize the state count with the total number of states\n",
    "    start_prob[state] = state_count[state] / total_states\n",
    "    \n",
    "print(start_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-handbook",
   "metadata": {},
   "source": [
    "Next, we can obtain the transition probabilities by calculating the frequency of the state transitions\n",
    "on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adopted-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': {'B': 0.0, 'I': 0.13515948819607138, 'E': 0.8648405118039286, 'S': 0.0}, 'I': {'B': 0.0, 'I': 0.18824410956623094, 'E': 0.8117558904337691, 'S': 0.0}, 'E': {'B': 0.5855109028653811, 'I': 0.0, 'E': 0.0, 'S': 0.3511928691240279}, 'S': {'B': 0.5720509604594363, 'I': 0.0, 'E': 0.0, 'S': 0.40755165357449336}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the transition probabilities\n",
    "trans_prob = {}\n",
    "for state in states:\n",
    "    trans_prob[state] = {}\n",
    "    for state_i in states:\n",
    "        trans_prob[state][state_i] = 0\n",
    "\n",
    "for i in range(len(train_hs)):\n",
    "    length = len(train_hs[i])\n",
    "    if length > 0:\n",
    "        for j in range(length - 1):\n",
    "            # Update the transition probabilities\n",
    "            s_from = train_hs[i][j]\n",
    "            s_to = train_hs[i][j+1]\n",
    "            trans_prob[s_from][s_to] += 1\n",
    "\n",
    "for i in states:\n",
    "    for j in states:\n",
    "        # Normalize the frequency of the transition with the state counts\n",
    "        trans_prob[i][j] /= float(state_count[i])\n",
    "\n",
    "print(trans_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-values",
   "metadata": {},
   "source": [
    "Finally, the emission probabilities can be calculated as:\n",
    "\\begin{align}\n",
    "    P(\\text{Observation | State } k) = \\frac{\\text{The frequency of the observation given state } k}{\\text{The total number of state } k}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "favorite-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the emission probabilities\n",
    "emission_prob = {}\n",
    "\n",
    "# Get all the vocabulary in the corpus (train and test sets)\n",
    "vocab = list(set(train_wc.keys()) | set(test_wc.keys()))\n",
    "\n",
    "# Initialize the emission probabilities\n",
    "for state in states:\n",
    "    emission_prob[state] = {}\n",
    "    for word in vocab:\n",
    "        emission_prob[state][word] = 1\n",
    "\n",
    "for i in range(len(train_hs)):\n",
    "    length = len(train_hs[i])\n",
    "    for j in range(length):\n",
    "        # Update the emission probabilities\n",
    "        obs = train_data[i][j]\n",
    "        hidden = train_hs[i][j]\n",
    "        emission_prob[hidden][obs] += 1\n",
    "\n",
    "for state in states:\n",
    "    for word in vocab:\n",
    "        if emission_prob[state][word] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # Normalize the emission probabilities\n",
    "            emission_prob[state][word] /= float(state_count[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-oracle",
   "metadata": {},
   "source": [
    "After calculating the start, transition, and emission probabilities, we can utilize the Viterbi algorithm to label the states in the testing data. Given the sequence of observations, the Viterbi algorithm obtains the maximum a posteriori probability estimate of the most likely sequence of hidden states. \n",
    "\n",
    "Initially, the Viterbi algorithm will calculate the probabilities of the initial state which is the product of the start probabilities and the emission probabilities given the first observation. For each observation step, the Viterbi algorithm will calculate these probabilities until the last observation. Finally, it will perform backtrack to get the most probable state at each step. This is also known as the Viterbi path, which will be the output of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "imposed-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decoding(obs):\n",
    "    obs = [i for i in obs if i]\n",
    "    if len(obs) > 0:\n",
    "        # Initialize a list of dictionary to store the probabilites\n",
    "        V = [{}]\n",
    "        for st in states:\n",
    "            # Append the initial probabilites\n",
    "            V[0][st] = {\"prob\": start_prob[st] * emission_prob[st][obs[0]], \"prev\": None}\n",
    "        for t in range(1, len(obs)):\n",
    "            # Append a dictionary to store the probabilities at time/step t\n",
    "            V.append({})\n",
    "            for st in states:\n",
    "                max_tr_prob = V[t - 1][states[0]][\"prob\"] * trans_prob[states[0]][st]\n",
    "                prev_st_selected = states[0]\n",
    "                for prev_st in states[1:]:\n",
    "                    # Calculate the probabilities of each state\n",
    "                    tr_prob = V[t - 1][prev_st][\"prob\"] * trans_prob[prev_st][st]\n",
    "                    if tr_prob > max_tr_prob:\n",
    "                        max_tr_prob = tr_prob\n",
    "                        prev_st_selected = prev_st\n",
    "\n",
    "                # Get the max probability at time/step t\n",
    "                max_prob = max_tr_prob * emission_prob[st][obs[t]]\n",
    "                V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "\n",
    "        path = []\n",
    "        max_prob = -float(\"inf\")\n",
    "        best_st = None\n",
    "        # Get most probable state and its backtrack\n",
    "        for st, data in V[-1].items():\n",
    "            if data[\"prob\"] > max_prob:\n",
    "                max_prob = data[\"prob\"]\n",
    "                best_st = st\n",
    "        path.append(best_st)\n",
    "        previous = best_st\n",
    "\n",
    "        # Follow the backtrack till the first observation\n",
    "        for t in range(len(V) - 2, -1, -1):\n",
    "            path.insert(0, V[t + 1][previous][\"prev\"])\n",
    "            previous = V[t + 1][previous][\"prev\"]\n",
    "\n",
    "        # Return the path\n",
    "        return path\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-loading",
   "metadata": {},
   "source": [
    "We will use the above Viterbi algorithm to label the sentences (sequences) in the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "streaming-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = []\n",
    "for obs in test_data:\n",
    "    # Label the sequence using Viterbi algorithm\n",
    "    test_pred.append(viterbi_decoding(obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-recycling",
   "metadata": {},
   "source": [
    "After obtaining the labels for the test data, we can compute the $F_1$ score to evaluate the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "impaired-weight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for state B: 0.5777712683600452\n",
      "F1 score for state I: 0.15008726003490402\n",
      "F1 score for state E: 0.5862872146698852\n",
      "F1 score for state S: 0.3726343144455116\n",
      "Macro-F1 score: 0.4216950143775865\n"
     ]
    }
   ],
   "source": [
    "# Create mapping for each label\n",
    "map_dict = {\n",
    "    \"B\": 0,\n",
    "    \"I\": 1,\n",
    "    \"E\": 2,\n",
    "    \"S\": 3\n",
    "}\n",
    "\n",
    "# Flatten the lists into a 1D list\n",
    "true = list(np.concatenate(test_hs).flat)\n",
    "pred = list(np.concatenate(test_pred).flat)\n",
    "\n",
    "k = len(np.unique(true)) # Number of classes\n",
    "result = np.zeros((k, k)) # Initialize the confusion matrix\n",
    "\n",
    "for i in range(len(true)):\n",
    "    # Calculate the confusion matrix\n",
    "    result[map_dict[true[i]]][map_dict[pred[i]]] += 1\n",
    "\n",
    "# Calculate precision for each label\n",
    "precision_B = result[0][0] / (result[0][0] + result[1][0] + result[2][0] + result[3][0])\n",
    "precision_I = result[1][1] / (result[0][1] + result[1][1] + result[2][1] + result[3][1])\n",
    "precision_E = result[2][2] / (result[0][2] + result[1][2] + result[2][2] + result[3][2])\n",
    "precision_S = result[3][3] / (result[0][3] + result[1][3] + result[2][3] + result[3][3])\n",
    "\n",
    "# Calculate recall for each label\n",
    "recall_B = result[0][0] / (result[0][0] + result[0][1] + result[0][2] + result[0][3])\n",
    "recall_I = result[1][1] / (result[1][0] + result[1][1] + result[1][2] + result[1][3])\n",
    "recall_E = result[2][2] / (result[2][0] + result[2][1] + result[2][2] + result[2][3])\n",
    "recall_S = result[3][3] / (result[3][0] + result[3][1] + result[3][2] + result[3][3])\n",
    "\n",
    "# Calculate F1 for each label\n",
    "f1_B = 2 *(precision_B * recall_B)/(precision_B + recall_B)\n",
    "f1_I = 2 *(precision_I * recall_I)/(precision_I + recall_I)\n",
    "f1_E = 2 *(precision_E * recall_E)/(precision_E + recall_E)\n",
    "f1_S = 2 *(precision_S * recall_S)/(precision_S + recall_S)\n",
    "\n",
    "# Calculate Macro-F1\n",
    "macro_f1 = (f1_B + f1_I + f1_E + f1_S) / k\n",
    "\n",
    "print(f\"F1 score for state B: {f1_B}\")\n",
    "print(f\"F1 score for state I: {f1_I}\")\n",
    "print(f\"F1 score for state E: {f1_E}\")\n",
    "print(f\"F1 score for state S: {f1_S}\")\n",
    "print(f\"Macro-F1 score: {macro_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-essex",
   "metadata": {},
   "source": [
    "Overall, we can observe that most of the misclassification occurs for state 'I', which is the intermediate state. This might be because the intial probability and the transition probability of state 'I' are significantly lower than the other states, i.e., the state 'I' is quite rare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-illness",
   "metadata": {},
   "source": [
    "Finally, we can apply the word segmentation according to the predicted states for the sequence of testinh data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "integrated-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_segmentation(test_data, pred):\n",
    "    segmented = \"\"\n",
    "    i = 0 # Counter for the test data index\n",
    "    j = 0 # Counter for the prediction index\n",
    "    while i < len(test_data):\n",
    "        segmented += test_data[i] \n",
    "        # Check for Chinese character\n",
    "        if test_data[i] > u\"\\u4e00\" and test_data[i] < u\"\\u9fff\":\n",
    "            # Add space after the character if label\n",
    "            # is either \"E\" or \"S\"\n",
    "            if pred[j] in [\"E\", \"S\"]:\n",
    "                segmented += \" \"\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return segmented\n",
    "\n",
    "segmented = []\n",
    "for i in range(len(test_data)):\n",
    "    # Convert BIES to word segmentation\n",
    "    segmented.append(word_segmentation(test_data[i], test_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "leading-tactics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這 是 記者 近日 從 浙江省 政府 了解 到 的 。\n",
      "['S', 'S', 'B', 'E', 'B', 'E', 'B', 'I', 'I', 'E', 'B', 'E', 'S', 'B', 'E', 'B']\n",
      "這 是 記者 近日 從浙江省 政府 了 解到 的\n"
     ]
    }
   ],
   "source": [
    "# Look at random segmented sentence\n",
    "i = np.random.randint(0, len(test_gold))\n",
    "print(test_label[i])\n",
    "print(test_pred[i])\n",
    "print(segmented[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "opponent-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the segmented sentences to my_prediction.txt file\n",
    "with open(\"my_prediction.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    for item in segmented:\n",
    "        fout.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondae4d1925b83da41c78817766fdb596e37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
